{"ast":null,"code":"'use strict';\n\nexports.__esModule = true;\nexports.tokenize = exports.test = exports.scanner = exports.parser = exports.options = exports.inherits = exports.find = undefined;\n\nvar _class = require('./linkify/utils/class');\n\nvar _options = require('./linkify/utils/options');\n\nvar options = _interopRequireWildcard(_options);\n\nvar _scanner = require('./linkify/core/scanner');\n\nvar scanner = _interopRequireWildcard(_scanner);\n\nvar _parser = require('./linkify/core/parser');\n\nvar parser = _interopRequireWildcard(_parser);\n\nfunction _interopRequireWildcard(obj) {\n  if (obj && obj.__esModule) {\n    return obj;\n  } else {\n    var newObj = {};\n\n    if (obj != null) {\n      for (var key in obj) {\n        if (Object.prototype.hasOwnProperty.call(obj, key)) newObj[key] = obj[key];\n      }\n    }\n\n    newObj.default = obj;\n    return newObj;\n  }\n}\n\nif (!Array.isArray) {\n  Array.isArray = function (arg) {\n    return Object.prototype.toString.call(arg) === '[object Array]';\n  };\n}\n/**\n\tConverts a string into tokens that represent linkable and non-linkable bits\n\t@method tokenize\n\t@param {String} str\n\t@return {Array} tokens\n*/\n\n\nvar tokenize = function tokenize(str) {\n  return parser.run(scanner.run(str));\n};\n/**\n\tReturns a list of linkable items in the given string.\n*/\n\n\nvar find = function find(str) {\n  var type = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : null;\n  var tokens = tokenize(str);\n  var filtered = [];\n\n  for (var i = 0; i < tokens.length; i++) {\n    var token = tokens[i];\n\n    if (token.isLink && (!type || token.type === type)) {\n      filtered.push(token.toObject());\n    }\n  }\n\n  return filtered;\n};\n/**\n\tIs the given string valid linkable text of some sort\n\tNote that this does not trim the text for you.\n\n\tOptionally pass in a second `type` param, which is the type of link to test\n\tfor.\n\n\tFor example,\n\n\t\ttest(str, 'email');\n\n\tWill return `true` if str is a valid email.\n*/\n\n\nvar test = function test(str) {\n  var type = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : null;\n  var tokens = tokenize(str);\n  return tokens.length === 1 && tokens[0].isLink && (!type || tokens[0].type === type);\n}; // Scanner and parser provide states and tokens for the lexicographic stage\n// (will be used to add additional link types)\n\n\nexports.find = find;\nexports.inherits = _class.inherits;\nexports.options = options;\nexports.parser = parser;\nexports.scanner = scanner;\nexports.test = test;\nexports.tokenize = tokenize;","map":{"version":3,"sources":["/home/rupak/Music/project_medical_pager_chat-master/client/node_modules/linkifyjs/lib/linkify.js"],"names":["exports","__esModule","tokenize","test","scanner","parser","options","inherits","find","undefined","_class","require","_options","_interopRequireWildcard","_scanner","_parser","obj","newObj","key","Object","prototype","hasOwnProperty","call","default","Array","isArray","arg","toString","str","run","type","arguments","length","tokens","filtered","i","token","isLink","push","toObject"],"mappings":"AAAA;;AAEAA,OAAO,CAACC,UAAR,GAAqB,IAArB;AACAD,OAAO,CAACE,QAAR,GAAmBF,OAAO,CAACG,IAAR,GAAeH,OAAO,CAACI,OAAR,GAAkBJ,OAAO,CAACK,MAAR,GAAiBL,OAAO,CAACM,OAAR,GAAkBN,OAAO,CAACO,QAAR,GAAmBP,OAAO,CAACQ,IAAR,GAAeC,SAAzH;;AAEA,IAAIC,MAAM,GAAGC,OAAO,CAAC,uBAAD,CAApB;;AAEA,IAAIC,QAAQ,GAAGD,OAAO,CAAC,yBAAD,CAAtB;;AAEA,IAAIL,OAAO,GAAGO,uBAAuB,CAACD,QAAD,CAArC;;AAEA,IAAIE,QAAQ,GAAGH,OAAO,CAAC,wBAAD,CAAtB;;AAEA,IAAIP,OAAO,GAAGS,uBAAuB,CAACC,QAAD,CAArC;;AAEA,IAAIC,OAAO,GAAGJ,OAAO,CAAC,uBAAD,CAArB;;AAEA,IAAIN,MAAM,GAAGQ,uBAAuB,CAACE,OAAD,CAApC;;AAEA,SAASF,uBAAT,CAAiCG,GAAjC,EAAsC;AAAE,MAAIA,GAAG,IAAIA,GAAG,CAACf,UAAf,EAA2B;AAAE,WAAOe,GAAP;AAAa,GAA1C,MAAgD;AAAE,QAAIC,MAAM,GAAG,EAAb;;AAAiB,QAAID,GAAG,IAAI,IAAX,EAAiB;AAAE,WAAK,IAAIE,GAAT,IAAgBF,GAAhB,EAAqB;AAAE,YAAIG,MAAM,CAACC,SAAP,CAAiBC,cAAjB,CAAgCC,IAAhC,CAAqCN,GAArC,EAA0CE,GAA1C,CAAJ,EAAoDD,MAAM,CAACC,GAAD,CAAN,GAAcF,GAAG,CAACE,GAAD,CAAjB;AAAyB;AAAE;;AAACD,IAAAA,MAAM,CAACM,OAAP,GAAiBP,GAAjB;AAAsB,WAAOC,MAAP;AAAgB;AAAE;;AAE7Q,IAAI,CAACO,KAAK,CAACC,OAAX,EAAoB;AACnBD,EAAAA,KAAK,CAACC,OAAN,GAAgB,UAAUC,GAAV,EAAe;AAC9B,WAAOP,MAAM,CAACC,SAAP,CAAiBO,QAAjB,CAA0BL,IAA1B,CAA+BI,GAA/B,MAAwC,gBAA/C;AACA,GAFD;AAGA;AAED;AACA;AACA;AACA;AACA;AACA;;;AACA,IAAIxB,QAAQ,GAAG,SAASA,QAAT,CAAkB0B,GAAlB,EAAuB;AACrC,SAAOvB,MAAM,CAACwB,GAAP,CAAWzB,OAAO,CAACyB,GAAR,CAAYD,GAAZ,CAAX,CAAP;AACA,CAFD;AAIA;AACA;AACA;;;AACA,IAAIpB,IAAI,GAAG,SAASA,IAAT,CAAcoB,GAAd,EAAmB;AAC7B,MAAIE,IAAI,GAAGC,SAAS,CAACC,MAAV,GAAmB,CAAnB,IAAwBD,SAAS,CAAC,CAAD,CAAT,KAAiBtB,SAAzC,GAAqDsB,SAAS,CAAC,CAAD,CAA9D,GAAoE,IAA/E;AAEA,MAAIE,MAAM,GAAG/B,QAAQ,CAAC0B,GAAD,CAArB;AACA,MAAIM,QAAQ,GAAG,EAAf;;AAEA,OAAK,IAAIC,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGF,MAAM,CAACD,MAA3B,EAAmCG,CAAC,EAApC,EAAwC;AACvC,QAAIC,KAAK,GAAGH,MAAM,CAACE,CAAD,CAAlB;;AACA,QAAIC,KAAK,CAACC,MAAN,KAAiB,CAACP,IAAD,IAASM,KAAK,CAACN,IAAN,KAAeA,IAAzC,CAAJ,EAAoD;AACnDI,MAAAA,QAAQ,CAACI,IAAT,CAAcF,KAAK,CAACG,QAAN,EAAd;AACA;AACD;;AAED,SAAOL,QAAP;AACA,CAdD;AAgBA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;AACA,IAAI/B,IAAI,GAAG,SAASA,IAAT,CAAcyB,GAAd,EAAmB;AAC7B,MAAIE,IAAI,GAAGC,SAAS,CAACC,MAAV,GAAmB,CAAnB,IAAwBD,SAAS,CAAC,CAAD,CAAT,KAAiBtB,SAAzC,GAAqDsB,SAAS,CAAC,CAAD,CAA9D,GAAoE,IAA/E;AAEA,MAAIE,MAAM,GAAG/B,QAAQ,CAAC0B,GAAD,CAArB;AACA,SAAOK,MAAM,CAACD,MAAP,KAAkB,CAAlB,IAAuBC,MAAM,CAAC,CAAD,CAAN,CAAUI,MAAjC,KAA4C,CAACP,IAAD,IAASG,MAAM,CAAC,CAAD,CAAN,CAAUH,IAAV,KAAmBA,IAAxE,CAAP;AACA,CALD,C,CAOA;AACA;;;AACA9B,OAAO,CAACQ,IAAR,GAAeA,IAAf;AACAR,OAAO,CAACO,QAAR,GAAmBG,MAAM,CAACH,QAA1B;AACAP,OAAO,CAACM,OAAR,GAAkBA,OAAlB;AACAN,OAAO,CAACK,MAAR,GAAiBA,MAAjB;AACAL,OAAO,CAACI,OAAR,GAAkBA,OAAlB;AACAJ,OAAO,CAACG,IAAR,GAAeA,IAAf;AACAH,OAAO,CAACE,QAAR,GAAmBA,QAAnB","sourcesContent":["'use strict';\n\nexports.__esModule = true;\nexports.tokenize = exports.test = exports.scanner = exports.parser = exports.options = exports.inherits = exports.find = undefined;\n\nvar _class = require('./linkify/utils/class');\n\nvar _options = require('./linkify/utils/options');\n\nvar options = _interopRequireWildcard(_options);\n\nvar _scanner = require('./linkify/core/scanner');\n\nvar scanner = _interopRequireWildcard(_scanner);\n\nvar _parser = require('./linkify/core/parser');\n\nvar parser = _interopRequireWildcard(_parser);\n\nfunction _interopRequireWildcard(obj) { if (obj && obj.__esModule) { return obj; } else { var newObj = {}; if (obj != null) { for (var key in obj) { if (Object.prototype.hasOwnProperty.call(obj, key)) newObj[key] = obj[key]; } } newObj.default = obj; return newObj; } }\n\nif (!Array.isArray) {\n\tArray.isArray = function (arg) {\n\t\treturn Object.prototype.toString.call(arg) === '[object Array]';\n\t};\n}\n\n/**\n\tConverts a string into tokens that represent linkable and non-linkable bits\n\t@method tokenize\n\t@param {String} str\n\t@return {Array} tokens\n*/\nvar tokenize = function tokenize(str) {\n\treturn parser.run(scanner.run(str));\n};\n\n/**\n\tReturns a list of linkable items in the given string.\n*/\nvar find = function find(str) {\n\tvar type = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : null;\n\n\tvar tokens = tokenize(str);\n\tvar filtered = [];\n\n\tfor (var i = 0; i < tokens.length; i++) {\n\t\tvar token = tokens[i];\n\t\tif (token.isLink && (!type || token.type === type)) {\n\t\t\tfiltered.push(token.toObject());\n\t\t}\n\t}\n\n\treturn filtered;\n};\n\n/**\n\tIs the given string valid linkable text of some sort\n\tNote that this does not trim the text for you.\n\n\tOptionally pass in a second `type` param, which is the type of link to test\n\tfor.\n\n\tFor example,\n\n\t\ttest(str, 'email');\n\n\tWill return `true` if str is a valid email.\n*/\nvar test = function test(str) {\n\tvar type = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : null;\n\n\tvar tokens = tokenize(str);\n\treturn tokens.length === 1 && tokens[0].isLink && (!type || tokens[0].type === type);\n};\n\n// Scanner and parser provide states and tokens for the lexicographic stage\n// (will be used to add additional link types)\nexports.find = find;\nexports.inherits = _class.inherits;\nexports.options = options;\nexports.parser = parser;\nexports.scanner = scanner;\nexports.test = test;\nexports.tokenize = tokenize;"]},"metadata":{},"sourceType":"script"}